---
title: "SYS865 Inférence statistique avec programmation R"
author: "Ornwipa Thamsuwan"
date: "13 mars 2024"
# date: "`r format(Sys.time(), '%d-%m-%Y')`"
output:
  beamer_presentation: 
    slide_level: 2
    theme: "Goettingen"
    colortheme: "crane"
    fonttheme: "structurebold"
header-includes:
- \setbeamertemplate{navigation symbols}{}
- \setbeamertemplate{footline}[page number]
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# Plan de la séance

- Corrélation
- Régression linéaire

![Brise-glace](Slide-images\constellation.jpg){width=50%, height=50%}

# Corrélation

La corrélation de variables aléatoires est une mesure qui quantifie le degré auquel deux variables aléatoires varient ensemble. 

- Si les variations des deux variables montrent une tendance à se produire ensemble, on dit qu'elles sont positivement corrélées. 
- Si une variable a tendance à augmenter quand l'autre diminue, elles sont négativement corrélées. 

\pause

La corrélation est souvent mesurée par un **coefficient** qui varie entre -1 et 1. 

Un coefficient de 1 indique une corrélation positive parfaite, -1 indique une corrélation négative parfaite, et 0 indique l'absence de corrélation.

## Corrélation de Pearson (Paramétrique)

Définition: La corrélation de Pearson, également connue sous le nom de coefficient de corrélation produit-moment de Pearson, évalue la **relation linéaire** entre deux variables quantitatives.

Caractéristiques: Valeurs entre -1 et 1.

Utilisation: Préférable lorsque les deux variables sont **normalement distribuées** et la relation est supposée être linéaire.

## Corrélation de Pearson (Paramétrique)

Formule: Corrélation de Pearson = (Covariance de X et Y) / (Écart-type de X * Écart-type de Y).

$$
r_{xy} = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2} \sum{(y_i - \bar{y})^2}}}
$$

où \( r_{xy} \) est le coefficient de corrélation de Pearson entre les variables \( x \) et \( y \), \( x_i \) et \( y_i \) sont les valeurs des variables, et \( \bar{x} \) et \( \bar{y} \) sont les moyennes de \( x \) et \( y \), respectivement.

## Corrélation de Spearman (Non-Paramétrique)

Définition: La corrélation de Spearman, ou le coefficient de **rang** de Spearman, est utilisée pour mesurer la force et la direction de l'association entre deux variables classées.

Caractéristiques: Également évaluée entre -1 et 1. Moins sensible aux valeurs aberrantes.

Utilisation: Appropriée lorsque les données ne sont pas normalement distribuées ou lorsqu'on examine des relations non linéaires.

## Corrélation de Spearman (Non-Paramétrique)

Formule: Corrélation de Spearman = 1 - (6 * Somme des carrés des différences de rang) / (n(n^2 - 1)).

$$
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
$$

où \( \rho \) est le coefficient de corrélation de Spearman, \( d_i \) est la différence entre les rangs des i-èmes valeurs de \( x \) et \( y \), et \( n \) est le nombre de paires de données.

## Rappel

Il est crucial de se rappeler que \textcolor{red}{la corrélation ne signifie pas causalité.} 

![Corrélation vs. causalité](Slide-images\breathe.jpg){width=45%, height=45%}

## Analyse avec R : Données

Base de données "Pima Indian Diabetes" 

\pause
**Test de normalité de Shapiro-Wilk**
```{r, echo=FALSE}
# Read the dataset
diabetes <- read.csv("diabetes.csv")

# Exclude columns 1 and 7
diabetes_subset <- diabetes[, -c(1, 7, 9)]

# Replace zeros with NA (missing values) in the dataset
diabetes_subset[diabetes_subset == 0] <- NA

# Perform Shapiro-Wilk test for each variable
for (variable in names(diabetes_subset)) {
  shapiro_test <- shapiro.test(diabetes_subset[[variable]])
  p_value <- shapiro_test$p.value
  cat(variable, ": p-value =", p_value, "\n")
}
```
\pause
Les données ne sont pas normalement distribuées. Il faut donc utiliser la corrélation de Spearman.

## Analyse avec R : Histogrammes

```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(2, 3))
for (variable in names(diabetes_subset)) {
  hist(diabetes_subset[[variable]], main=variable, xlab="", ylab="", na.rm=TRUE)
}
```

## Analyse avec R : Graphiques Q-Q

```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(2, 3))
for (variable in names(diabetes_subset)) {
  qqnorm(diabetes_subset[[variable]], main=variable, na.rm=TRUE)
  qqline(diabetes_subset[[variable]], col="red", na.rm=TRUE)
}
```

## Analyse avec R : Corrélation de Spearman

```{r}
spearman_correlation_matrix <- 
  cor(diabetes_subset, 
      use="complete.obs", 
      method="spearman")
```
La fonction `cor(diabetes_subset)` calcule les coefficients de corrélation pour toutes les paires de variables dans la base de données `diabetes_subset`.

L'argument `method="spearman"` spécifie que le coefficient de corrélation de rang de Spearman doit être utilisé.

L'argument `use="complete.obs"` indique à R d'utiliser uniquement des cas complets (c'est-à-dire des lignes sans aucune valeur NA).

## Analyse avec R : Corrélation de Spearman

```{r, echo=FALSE}
par(mar = c(5, 5, 4, 2) + 0.1)

# Number of variables
num_vars <- ncol(diabetes_subset)

# Create the heatmap
image(1:num_vars, 1:num_vars, t(spearman_correlation_matrix), col = heat.colors(256), xlab = "", ylab = "", axes = FALSE, main = "Spearman Correlation Matrix")

# Add correlation values
for (i in 1:num_vars) {
  for (j in 1:num_vars) {
    correlation_value <- round(spearman_correlation_matrix[j, i], 2) 
    text(i, j, correlation_value, cex = 1.5) 
  }
}

# Add variable names as labels
axis(1, at = 1:num_vars, labels = names(diabetes_subset), las=2, cex.axis = 1.1)
axis(2, at = 1:num_vars, labels = names(diabetes_subset), las=2, cex.axis = 1.1)

par(mar = c(5, 4, 4, 2) + 0.1)
```

- Assez forte corrélation positive entre `SkinThickness` et `BMI`, et entre `Glucose` et `Insulin`. Toutefois, ...

## Rappel

Le fait que deux variables soient fortement corrélées ne démontre pas que l'une est la cause de l'autre.

![Corrélation fallacieuse](Slide-images\spurious-correlation.png){width=50%, height=50%}

## Analyse avec R : Visualisation de données

Iris - nuage de points ("scatter plots" en anglais)

```{r, echo=FALSE}
par(mfrow = c(2, 2))

plot(iris$Petal.Length, iris$Petal.Width, col = iris$Species, 
     xlab = "Longueur des pétales", ylab = "Largeur des pétales", 
     main = "Pétales: Largeur vs Longueur")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, 
     xlab = "Longueur des sépales", ylab = "Largeur des sépales", 
     main = "Sépales: Largeur vs Longueur")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Petal.Length, iris$Sepal.Length, col = iris$Species, 
     xlab = "Longueur des pétales", ylab = "Longueur des sépales", 
     main = "Longeurs: Pétale vs Sépale")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Petal.Width, iris$Sepal.Width, col = iris$Species, 
     xlab = "Largeur des pétales", ylab = "Largeur des sépales", 
     main = "Largeur: Pétale vs Sépale")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

par(mfrow = c(1, 1))
```

## Analyse avec R : Distribution normale

**Test de normalité de Shapiro-Wilk**
```{r, echo=FALSE}
variables <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")

for (variable in variables) {
  test_result <- shapiro.test(iris[[variable]])
  cat(variable, ": p-value =", round(test_result$p.value, 5), "\n")
}
```

\pause
En incluant uniquement l'espèce de `setosa`
```{r, echo=FALSE}
iris_setosa <- subset(iris, Species == "setosa")

for (variable in variables) {
  test_result <- shapiro.test(iris_setosa[[variable]])
  cat(variable, ": p-value =", round(test_result$p.value, 5), "\n")
}
```

## Analyse avec R : Rélation linéaire

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width, 
     main = "Nuage de points - Iris Setosa",
     xlab = "Longueur du Sépale (cm)", ylab = "Largeur du Sépale (cm)",
     col = "blue", pch = 19)
```

\pause
Ainsi, nous démontrerons le calcul de corrélation de Pearson pour la longueur et la largeur des sépales de `setosa`.

## Analyse avec R : Corrélation de Pearson

La méthode par défaut pour `cor()` est le coefficient de corrélation de Pearson.

La fonction `cor(x, y)` ou `cor(x, y, method = "pearson")` calcule le coefficient de corrélation de Pearson entre deux variables `x` et `y`.

```{r}
cor(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width)
```

\pause
- Le coefficient d'environ 0,74 suggère qu'à mesure que l'une des variables (longueur ou largeur) augmente, l'autre variable a tendance à augmenter également, et cette relation est relativement forte. Cependant, ...

## Rappel

L'existence d'une corrélation entre deux variables n'implique pas une relation de cause à effet.

![Corrélation, et non causalité](Slide-images\cat-correlation.jpg){width=50%, height=50%}

# Régression linéaire simple

Alors, peut-on connaître ou deviner la largeur des sépales si l'on a déjà mesuré la longueur des sépales ? 

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width, 
     main = "Nuage de points - Iris Setosa",
     xlab = "Longueur du Sépale (cm)", ylab = "Largeur du Sépale (cm)",
     col = "blue", pch = 19)

# Fit a linear model
model <- lm(Sepal.Width ~ Sepal.Length, data = iris_setosa)

# Add a linear regression line
abline(model, col = "red")
```

## Relation entre la corrélation de Pearson et la régression linéaire simple

**But**

- **Corrélation de Pearson** mesure la force et la direction de la relation linéaire entre deux variables.

- **Régression linéaire** explique une variable (réponse) en fonction de la valeur d'une autre (prédicteur).

\pause

**Résultat**

- **Coefficient de corrélation** (`r`) varie de -1 à 1.

- **Régression linéaire** fournit une équation de la forme : \( y = \beta_0 + \beta_1 x + \epsilon \)

## Régression linéaire simple : Modèle

La régression linéaire simple d'une variable dépendante \( y \) sur une variable indépendante \( x \) est modélisée par: 
$$
y = \beta_0 + \beta_1 x + \epsilon
$$

  - Intercept (\( \beta_0 \)) est l'ordonnée à l'origine ou la valeur attendue de \( y \) quand \( x=0 \).
  
  - Pente (\( \beta_1 \)) est la pente de la ligne de régression indiquant le changement attendu dans \( y \) pour une augmentation d'une unité de \( x \).
  
  - Terme d'erreur (\( \epsilon \)) est la variation non expliquée.
  
## Régression linéaire simple : Estimation des coefficients

L'objectif est d'estimer les coefficients \( \beta_0 \) et \( \beta_1 \) à partir des données. Cela se fait généralement en utilisant la méthode des **moindres carrés**, qui minimise la somme des différences au carré entre les valeurs observées et les valeurs prédites par le modèle.

\pause
- Calcul des résidus : \( \epsilon_i = y_i - (\beta_0 + \beta_1 x_i) \)
  - \( \epsilon_i \) est le résidu pour l'observation \( i \)
  - \( y_i \) est la valeur observée
  - \( (\beta_0 + \beta_1 x_i) \) est la valeur prédite par le modèle

\pause
- Minimisation de la a somme des carrés des résidus : \( \sum \epsilon_i^2 = \sum (y_i - (\beta_0 + \beta_1 x_i))^2 \)

- Trouve les coefficients \( \beta_0 \) et \( \beta_1 \) qui minimisent cette somme

## Régression linéaire simple : Conditions

- **Linéarité** : La relation entre `x` et `y` est linéaire.
- **Indépendance** : Les observations sont indépendantes.
- **Homoscédasticité** : La variance du terme d'erreur \(\epsilon\) est constante pour tous les niveaux de `x`.
- **Normalité des erreurs** : Les termes d'erreur \(\epsilon\) sont normalement distribués (important pour faire des inférences sur les coefficients).

## Régression linéaire simple : Inférence

- **Tests d'Hypothèse** pour la pente (\( \beta_1 \)) en utilisant un test t, si elle est significativement différente de zéro.

\pause

- **Intervalles de Confiance** pour les coefficients \( \beta_0 \) et \( \beta_1 \).

\pause

- **Coefficient de Détermination (R²)** indique la qualité de l'ajustement du modèle aux données. 

  - Est équivalent du carré du coefficient de corrélation de Pearson (r²) 
  - Estime la proportion de la variance dans la variable dépendante `y` qui peut être prédite ou inférée à partir de la variable indépendante `x`.

## Relation entre la corrélation de Pearson et la régression linéaire simple (retour)

**Force de l'association** : La corrélation de Pearson fournit une mesure de la \textcolor{brown}{force de la relation} linéaire, ce qui est crucial pour \textcolor{brown}{décider si la régression linéaire est appropriée}.
\pause

**Direction et Pente** : Le signe du coefficient de corrélation de Pearson `r` indique la \textcolor{brown}{direction de la relation} (+ ou -), qui correspond à la \textcolor{brown}{pente dans la régression} linéaire.
\pause

**Variance expliquée** : Dans une régression linéaire simple avec un seul prédicteur, le carré du coefficient de corrélation de Pearson (r²) est égal à la statistique R² en régression, représentant la \textcolor{brown}{proportion de la variance dans la variable dépendante expliquée par la variable indépendante}.

## Analyse avec R : Modèle

Iris

**La pente de la régression / la direction de la rélation** : Le coefficient de `x` est-il positif ou négatif ?

```{r}
model <- lm(Sepal.Width ~ Sepal.Length, 
            data = iris_setosa)
model
```

## Analyse avec R : Tests d'Hypothèse

La valeur p du \(\beta_1\) (coefficient de `x`) est-elle inférieure à 0,05 ?

```{r}
summary(model)
```

## Analyse avec R : Intervalles de Confiance

Les intervalles de confiance couvrent-ils les valeurs négatives, 0, ou positives... ou toutes ?

```{r}
confint(model, level = 0.95)
```

## Analyse avec R : R²

**La force de l'association** : R² est-elle supérieure à 0,06 ?

```{r}
summary(model)$r.squared
```

\pause

Et le carré du coefficient de corrélation de Pearson est ...

```{r}
cor(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width)^2
```

## Analyse avec R : Homoscédasticité

La variance de \(\epsilon\) ou `model$residuals` est-elle constante pour tous les niveaux de `x` ?

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, model$residuals,
     xlab = "Sepal Length", ylab = "Residuals",
     main = "Residuals vs Sepal Length")
abline(h = 0, col = "red")
```

## Analyse avec R : Normalité des Erreurs

La valeur p du test Shapiro-Wilk des résidus du modèle (\(\epsilon\) ou `model$residuals`) est `r  shapiro.test(model$residuals)$p.value`.

```{r, echo=FALSE}
residuals <- resid(model)
qqnorm(residuals)
qqline(residuals, col = "red")
```

# Régression linéaire multiple

# Confondeurs