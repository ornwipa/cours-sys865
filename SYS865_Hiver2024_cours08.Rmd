---
title: "SYS865 Inférence statistique avec programmation R"
author: "Ornwipa Thamsuwan"
date: "13 mars 2024"
# date: "`r format(Sys.time(), '%d-%m-%Y')`"
output:
  beamer_presentation: 
    slide_level: 2
    theme: "Goettingen"
    colortheme: "crane"
    fonttheme: "structurebold"
header-includes:
- \setbeamertemplate{navigation symbols}{}
- \setbeamertemplate{footline}[page number]
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# Plan de la séance

- Corrélation
- Régression linéaire

![Brise-glace](Slide-images\constellation.jpg){width=50%, height=50%}

# Corrélation

La corrélation de variables aléatoires est une mesure qui quantifie le degré auquel deux variables aléatoires varient ensemble. 

- Si les variations des deux variables montrent une tendance à se produire ensemble, on dit qu'elles sont positivement corrélées. 
- Si une variable a tendance à augmenter quand l'autre diminue, elles sont négativement corrélées. 

\pause

La corrélation est souvent mesurée par un **coefficient** qui varie entre -1 et 1. 

Un coefficient de 1 indique une corrélation positive parfaite, -1 indique une corrélation négative parfaite, et 0 indique l'absence de corrélation.

## Corrélation

Fonction R : `cor(x, y = NULL, use = "everything", method = c("pearson", "kendall", "spearman"))`

\pause

Lorsqu'il manque des valeurs dans l'ensemble de données, il faut bien choisir l'argument `use` parmi `everythng`, `all.obs`,  `complete.obs`, `na.or.complete`, ou `pairwise.complete.obs`.

\pause

`method = c("pearson", "kendall", "spearman")`

Avec quelle méthode est-ce qu'on va utiliser ?

## Corrélation de Pearson (Paramétrique)

La corrélation de Pearson évalue la **relation linéaire** entre deux variables quantitatives.

Elle est utilisée lorsque les deux variables sont **normalement distribuées** et la relation est supposée être linéaire.

\pause

= (Covariance de X et Y) / (Écart-type de X * Écart-type de Y).

$$
r_{xy} = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2} \sum{(y_i - \bar{y})^2}}}
$$

où \( r_{xy} \) est le coefficient de corrélation de Pearson entre les variables \( x \) et \( y \), \( x_i \) et \( y_i \) sont les valeurs des variables, et \( \bar{x} \) et \( \bar{y} \) sont les moyennes de \( x \) et \( y \), respectivement.

## Corrélation de Spearman (Non-Paramétrique)

La corrélation de Spearman, ou le coefficient de **rang** de Spearman, est utilisée pour mesurer la force et la direction de l'association entre deux variables.

Elle est moins sensible aux valeurs aberrantes.

\pause

 = 1 - (6 * Somme des carrés des différences de rang) / (n(n^2 - 1)).

$$
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
$$

où \( \rho \) est le coefficient de corrélation de Spearman, \( d_i \) est la différence entre les rangs des i-èmes valeurs de \( x \) et \( y \), et \( n \) est le nombre de paires de données.

## Rappel

Il est crucial de se rappeler que \textcolor{red}{la corrélation ne signifie pas causalité.} 

![Corrélation vs. causalité](Slide-images\breathe.jpg){width=45%, height=45%}

## Analyse avec R : Données

Base de données "Pima Indian Diabetes" 

\pause
**Test de normalité de Shapiro-Wilk**
```{r, echo=FALSE}
# Read the dataset
diabetes <- read.csv("diabetes.csv")

# Exclude columns 1 and 7
diabetes_subset <- diabetes[, -c(1, 7, 9)]

# Replace zeros with NA (missing values) in the dataset
diabetes_subset[diabetes_subset == 0] <- NA

# Perform Shapiro-Wilk test for each variable
for (variable in names(diabetes_subset)) {
  shapiro_test <- shapiro.test(diabetes_subset[[variable]])
  p_value <- shapiro_test$p.value
  cat(variable, ": p-value =", p_value, "\n")
}
```
\pause
Les données ne sont pas normalement distribuées. On va donc utiliser la corrélation de Spearman.

## Analyse avec R : Histogrammes

```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(2, 3))
for (variable in names(diabetes_subset)) {
  hist(diabetes_subset[[variable]], main=variable, xlab="", ylab="", na.rm=TRUE)
}
```

## Analyse avec R : Graphiques Q-Q

```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(2, 3))
for (variable in names(diabetes_subset)) {
  qqnorm(diabetes_subset[[variable]], main=variable, na.rm=TRUE)
  qqline(diabetes_subset[[variable]], col="red", na.rm=TRUE)
}
```

## Analyse avec R : Corrélation de Spearman

```{r}
spearman_correlation_matrix <- 
  cor(diabetes_subset, 
      use="complete.obs", 
      method="spearman")
```
La fonction `cor(diabetes_subset)` calcule les coefficients de corrélation pour toutes les paires de variables dans la base de données `diabetes_subset`.

L'argument `method="spearman"` spécifie que le coefficient de corrélation de rang de Spearman doit être utilisé.

L'argument `use="complete.obs"` indique à R d'utiliser uniquement des cas complets (c'est-à-dire des lignes sans aucune valeur NA).

## Analyse avec R : Corrélation de Spearman

```{r, echo=FALSE}
par(mar = c(5, 5, 4, 2) + 0.1)

# Number of variables
num_vars <- ncol(diabetes_subset)

# Create the heatmap
image(1:num_vars, 1:num_vars, t(spearman_correlation_matrix), col = heat.colors(256), xlab = "", ylab = "", axes = FALSE, main = "Spearman Correlation Matrix")

# Add correlation values
for (i in 1:num_vars) {
  for (j in 1:num_vars) {
    correlation_value <- round(spearman_correlation_matrix[j, i], 2) 
    text(i, j, correlation_value, cex = 1.5) 
  }
}

# Add variable names as labels
axis(1, at = 1:num_vars, labels = names(diabetes_subset), las=2, cex.axis = 1.1)
axis(2, at = 1:num_vars, labels = names(diabetes_subset), las=2, cex.axis = 1.1)

par(mar = c(5, 4, 4, 2) + 0.1)
```

- Assez forte corrélation positive entre `SkinThickness` et `BMI`, et entre `Glucose` et `Insulin`. Toutefois, ...

## Rappel

Le fait que deux variables soient fortement corrélées ne démontre pas que l'une est la cause de l'autre.

![Corrélation fallacieuse](Slide-images\spurious-correlation.png){width=50%, height=50%}

## Analyse avec R : Visualisation de données

Iris - nuage de points ("scatter plots" en anglais)

```{r, echo=FALSE}
par(mfrow = c(2, 2))

plot(iris$Petal.Length, iris$Petal.Width, col = iris$Species, 
     xlab = "Longueur des pétales", ylab = "Largeur des pétales", 
     main = "Pétales: Largeur vs Longueur")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, 
     xlab = "Longueur des sépales", ylab = "Largeur des sépales", 
     main = "Sépales: Largeur vs Longueur")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Petal.Length, iris$Sepal.Length, col = iris$Species, 
     xlab = "Longueur des pétales", ylab = "Longueur des sépales", 
     main = "Longeurs: Pétale vs Sépale")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

plot(iris$Petal.Width, iris$Sepal.Width, col = iris$Species, 
     xlab = "Largeur des pétales", ylab = "Largeur des sépales", 
     main = "Largeur: Pétale vs Sépale")
legend("bottomright", legend = levels(iris$Species), col = 1:3, pch = 1)

par(mfrow = c(1, 1))
```

## Analyse avec R : Distribution normale

**Test de normalité de Shapiro-Wilk**
```{r, echo=FALSE}
variables <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")

for (variable in variables) {
  test_result <- shapiro.test(iris[[variable]])
  cat(variable, ": p-value =", round(test_result$p.value, 5), "\n")
}
```

\pause
En incluant uniquement l'espèce de `setosa`
```{r, echo=FALSE}
iris_setosa <- subset(iris, Species == "setosa")

for (variable in variables) {
  test_result <- shapiro.test(iris_setosa[[variable]])
  cat(variable, ": p-value =", round(test_result$p.value, 5), "\n")
}
```

## Analyse avec R : Rélation linéaire

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width, 
     main = "Nuage de points - Iris Setosa",
     xlab = "Longueur du Sépale (cm)", ylab = "Largeur du Sépale (cm)",
     col = "blue", pch = 19)
```

\pause
Ainsi, nous démontrerons le calcul de corrélation de Pearson pour la longueur et la largeur des sépales de `setosa`.

## Analyse avec R : Corrélation

La méthode par défaut pour la fonction `cor()` est le coefficient de corrélation de Pearson.

Lorsque les variables `x` et `y` sont normalement distribuées, on utilise la fonction `cor(x, y)` ou `cor(x, y, method = "pearson")` pour calculer le coefficient de corrélation de Pearson entre deux variables `x` et `y`.

```{r}
cor(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width)
```

\pause
- Le coefficient d'environ 0,74 suggère qu'à mesure que l'une des variables (longueur ou largeur) augmente, l'autre variable a tendance à augmenter également, et cette relation est relativement forte. Cependant, ...

## Rappel

L'existence d'une corrélation entre deux variables n'implique pas une relation de cause à effet.

![Corrélation, et non causalité](Slide-images\cat-correlation.jpg){width=50%, height=50%}

# Régression linéaire simple

Alors, peut-on connaître ou deviner la largeur des sépales si l'on a déjà mesuré la longueur des sépales ? 

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width, 
     main = "Nuage de points - Iris Setosa",
     xlab = "Longueur du Sépale (cm)", ylab = "Largeur du Sépale (cm)",
     col = "blue", pch = 19)

# Fit a linear model
model <- lm(Sepal.Width ~ Sepal.Length, data = iris_setosa)

# Add a linear regression line
abline(model, col = "red")
```

## Relation entre la corrélation de Pearson et la régression linéaire simple

**But**

- **Corrélation de Pearson** mesure la force et la direction de la relation linéaire entre deux variables.

- **Régression linéaire** explique une variable (réponse) en fonction de la valeur d'une autre (prédicteur).

\pause

**Résultat**

- **Coefficient de corrélation** (`r`) varie de -1 à 1.

- **Régression linéaire** fournit une équation de la forme : \( y = \beta_0 + \beta_1 x + \epsilon \)

## Régression linéaire simple : Modèle

La régression linéaire simple d'une variable dépendante \( y \) sur une variable indépendante \( x \) est modélisée par: 
$$
y = \beta_0 + \beta_1 x + \epsilon
$$

  - Ordonnée à l'origine (\( \beta_0 \)) est la valeur attendue de \( y \) quand \( x=0 \).
  
  - Pente (\( \beta_1 \)) est la pente de la ligne de régression indiquant le changement attendu dans \( y \) pour une augmentation d'une unité de \( x \).
  
  - Terme d'erreur (\( \epsilon \)) est la variation non expliquée.
  
## Régression linéaire simple : Estimation des coefficients

L'objectif est d'estimer les coefficients \( \beta_0 \) et \( \beta_1 \) à partir des données. Cela se fait généralement en utilisant la méthode des **moindres carrés**, qui minimise la somme des différences au carré entre les valeurs observées et les valeurs prédites par le modèle.

\pause
- Calcul des résidus : \( \epsilon_i = y_i - (\beta_0 + \beta_1 x_i) \)
  - \( \epsilon_i \) est le résidu pour l'observation \( i \)
  - \( y_i \) est la valeur observée
  - \( (\beta_0 + \beta_1 x_i) \) est la valeur prédite par le modèle

\pause
- Minimise la a somme des carrés des résidus : \( \sum \epsilon_i^2 = \sum (y_i - (\beta_0 + \beta_1 x_i))^2 \)

- Trouve les coefficients \( \beta_0 \) et \( \beta_1 \) qui minimisent cette somme

## Régression linéaire simple : Conditions

- **Linéarité** : La relation entre `x` et `y` est linéaire.
- **Indépendance** : Les observations sont indépendantes.
- **Homoscédasticité** : La variance du terme d'erreur \(\epsilon\) est constante pour tous les niveaux de `x`.
- **Normalité des erreurs** : Les termes d'erreur \(\epsilon\) sont normalement distribués (important pour faire des inférences sur les coefficients).

## Régression linéaire simple : Inférence

- **Tests d'Hypothèse** pour la pente (\( \beta_1 \)) en utilisant un test t, si elle est significativement différente de zéro.

\pause

- **Intervalles de Confiance** pour les coefficients \( \beta_0 \) et \( \beta_1 \).

\pause

- **Coefficient de Détermination (R²)** indique la qualité de l'ajustement du modèle aux données. 

  - Est équivalent du carré du coefficient de corrélation de Pearson (r²) 
  - Estime la proportion de la variance dans la variable dépendante `y` qui peut être prédite ou inférée à partir de la variable indépendante `x`.

## Relation entre la corrélation de Pearson et la régression linéaire simple (retour)

**Force de l'association** : La corrélation de Pearson fournit une mesure de la \textcolor{brown}{force de la relation} linéaire, ce qui est crucial pour \textcolor{brown}{décider si la régression linéaire est appropriée}.
\pause

**Direction et Pente** : Le signe du coefficient de corrélation de Pearson `r` indique la \textcolor{brown}{direction de la relation} (+ ou -), qui correspond à la \textcolor{brown}{pente dans la régression} linéaire.
\pause

**Variance expliquée** : Dans une régression linéaire simple avec un seul prédicteur, le carré du coefficient de corrélation de Pearson (r²) est égal à la statistique R² en régression, représentant la \textcolor{brown}{proportion de la variance dans la variable dépendante expliquée par la variable indépendante}.

## Analyse avec R : Modèle

Iris

**La pente de la régression / la direction de la rélation** : Le coefficient de `x` est-il positif ou négatif ?

```{r}
model <- lm(Sepal.Width ~ Sepal.Length, 
            data = iris_setosa)
model
```

## Analyse avec R : Tests d'Hypothèse

La valeur p du \(\beta_1\) (coefficient de `x`) est-elle inférieure à 0,05 ?

```{r}
summary(model)
```

## Analyse avec R : Intervalles de Confiance

Les intervalles de confiance couvrent-ils les valeurs négatives, 0, ou positives... ou toutes ?

```{r}
confint(model, level = 0.95)
```

## Analyse avec R : R²

**La force de l'association** : R² est-elle supérieure à 0,60 ?

```{r}
summary(model)$r.squared
```

\pause

Et le carré du coefficient de corrélation de Pearson est ...

```{r}
cor(iris_setosa$Sepal.Length, iris_setosa$Sepal.Width)^2
```

## Analyse avec R : Homoscédasticité

La variance de \(\epsilon\) ou `model$residuals` est-elle constante pour tous les niveaux de `x` ?

```{r, echo=FALSE}
plot(iris_setosa$Sepal.Length, model$residuals,
     xlab = "Sepal Length", ylab = "Residuals",
     main = "Residuals vs Sepal Length")
abline(h = 0, col = "red")
```

## Analyse avec R : Normalité des Erreurs

La valeur p du test Shapiro-Wilk des résidus du modèle (\(\epsilon\) ou `model$residuals`) est `r  round(shapiro.test(model$residuals)$p.value,2)`.

```{r, echo=FALSE}
residuals <- resid(model)
qqnorm(residuals)
qqline(residuals, col = "red")
```

# Régression linéaire multiple

![Brise-glace](Slide-images\elephant.jpg){width=70%, height=70%}

## Régression linéaire multiple

**Représentation du modèle**

- Rélation linéaire entre \( Y \) et plusieurs \( X_1, X_2, ..., X_k \)
- Formule : \( Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + \epsilon \)

\pause

**Estimation des coefficients**

- Utilisation de la méthode des moindres carrés.
- Minimisation de la somme des carrés des résidus.

\pause

**Interprétation des coefficients**

- \( \beta_0 \) : Valeur de \( Y \) lorsque toutes les \( X \) sont nulles
- \( \beta_1, \beta_2, ..., \beta_k \) : Effet de chaque \( X \) sur \( Y \)

## Régression linéaire multiple : Conditions

- Linéarité
- Indépendance
- Homoscédasticité
- Normalité des résidus
- **Absence de multicollinéarité** : Les variables indépendantes ne doivent pas être trop fortement corrélées entre elles.

## Régression linéaire multiple : Inférence

- Des **tests d'hypothèse** sont effectués pour déterminer si les coefficients sont significativement différents de zéro, ce qui indique que le prédicteur correspondant a un effet statistiquement significatif sur la variable dépendante.

\pause

- Des **intervalles de confiance** peuvent être construits pour les coefficients afin d'estimer leur précision.

\pause

### Évaluation et ajustement du modèle

- **R²** mesure la proportion de la variance de la variable dépendante expliquée par les variables indépendantes.
- **R² ajusté** est également utilisé, en particulier lors de la comparaison de modèles avec un nombre différent de prédicteurs.

## Régression linéaire multiple : R²

**Limitation du R²**

Un problème avec le R² est qu'il peut augmenter simplement en ajoutant plus de variables indépendantes au modèle, qu'elles soient significatives ou non. Cela peut conduire à un modèle surajusté (**overfitting**).

\pause

*Pour surmonter cette limitation ...*

Le R² ajusté modifie le R² pour prendre en compte le nombre de prédicteurs dans le modèle. 

## Régression linéaire multiple : R² ajusté

Le R² ajusté est calculé comme suit :

\[ R^2_{\text{ajusté}} = 1 - (1 - R^2) \times \frac{n - 1}{n - p - 1} \]

où :

- \( n \) est le nombre d'observations.
- \( p \) est le nombre de variables indépendantes.

\pause
\textcolor{red}{Le R² ajusté peut être inférieur au R², et contrairement au R², il ne va pas automatiquement augmenter avec l'ajout de nouvelles variables.}

## Analyse avec R : Données et modèle

Base de données "Pima Indian Diabetes" 

Notez que les données ne sont pas normalement distribuées. Cependant, à des fins de démonstration uniquement, on va utiliser les variables `Glucose`, `SkinThickness`, `Insulin`, `BMI` et `Age` pour prédire `BloodPressure`.

```{r, echo=FALSE}
diabetes <- read.csv("diabetes.csv")
diabetes_subset <- diabetes[, -c(1, 7, 9)]
data_filtered <- diabetes_subset[!apply(diabetes_subset, 1, function(x) any(x == 0)), ]
```
```{r}
model1 <- 
  lm(BloodPressure ~ Glucose + SkinThickness + 
       Insulin + BMI + Age, data = data_filtered)
```

\pause
Observez l'estimation et la valeur p pour chacun des \(\beta\)'s.

## Analyse avec R : Test d'hypothèse

```{r, echo=FALSE}
summary(model1)
```

## Analyse avec R : Interprétation, évaluation et ajustement

Les contributions de `SkinThickness` et `Insulin` au modèle ne sont pas significatives.

\pause

Selon l'analyse de corrélation ...

- `Glucose` et `Insulin` sont fortement corrélés
- `SkinThickness` et `BMI` sont fortement corrélés.

On peut choisir d'éliminer `SkinThickness` et `Insulin`.

```{r}
model2 <- 
  lm(BloodPressure ~ Glucose + BMI + Age, 
     data = data_filtered)
```

## Analyse avec R : Test d'hypothèse

```{r, echo=FALSE}
summary(model2)
```

## Analyse avec R : Intervalles de Confiance

```{r, echo=FALSE}
confint(model2)
```

On a observé que les intervalles de confiance des variables significatives (`BMI` et `Age`) ne couvrent pas 0, mais celui de la variable insignifiante (`Glucose`) le fait.

## Analyse avec R : R² et R² ajusté

```{r}
round(summary(model1)$r.squared,5)
round(summary(model1)$adj.r.squared,5)
round(summary(model2)$r.squared,5)
round(summary(model2)$adj.r.squared,5)
```

# Travaux pratiques

Continuez à travailler avec la base de données "Pima Indian Diabetes".

Définissez une variable dépendante et utilisez le reste comme variables indépendantes pour créer un modèle de régression linéaire.

Évaluez et ajustez le modèle pour atteindre un bon R².

Quelles variables apportent une contribution significative au modèle et dans quelle direction ?